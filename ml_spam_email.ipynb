{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO10R83pIAfn39sj8dBB9DZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarcusToledo/email-spam-classification-algorithms-comparison/blob/master/ml_spam_email.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkq1r0xcgbWc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "column_names = [\n",
        "    'word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our',\n",
        "    'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
        "    'word_freq_receive', 'word_freq_will', 'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
        "    'word_freq_free', 'word_freq_business', 'word_freq_email', 'word_freq_you', 'word_freq_credit',\n",
        "    'word_freq_your', 'word_freq_font', 'word_freq_000', 'word_freq_money', 'word_freq_hp',\n",
        "    'word_freq_hpl', 'word_freq_george', 'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
        "    'word_freq_telnet', 'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85',\n",
        "    'word_freq_technology', 'word_freq_1999', 'word_freq_parts', 'word_freq_pm', 'word_freq_direct',\n",
        "    'word_freq_cs', 'word_freq_meeting', 'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
        "    'word_freq_edu', 'word_freq_table', 'word_freq_conference', 'char_freq_;', 'char_freq_(',\n",
        "    'char_freq_[', 'char_freq_!', 'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
        "    'capital_run_length_longest', 'capital_run_length_total', 'label'\n",
        "]\n",
        "data = pd.read_csv(url, header=None, names=column_names)\n",
        "\n",
        "# Selecionando os atributos\n",
        "X = data.iloc[:, :-1].values  # Vou usar todos, o mínimo seria 7\n",
        "y = data.iloc[:, -1].values  # Selecionado a última coluna 'label'\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_ize=0.2, random_state=42)\n",
        "\n",
        "# Normalizar os dados\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "wVN8n60Agq_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de modelos\n",
        "models = {\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Support Vector Machine\": SVC(),\n",
        "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
        "    \"Neural Network\": MLPClassifier(hidden_layer_sizes=(12, 8), max_iter=1000, verbose=False)\n",
        "}\n",
        "\n",
        "# Armazenar os resultados\n",
        "results = []\n",
        "\n",
        "# Treinar e avaliar cada modelo\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Fazer previsões no conjunto de teste\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Avaliar o modelo\n",
        "    accuracy = accuracy_score(y_test, y_pred) * 100.0\n",
        "    accuracy_non_normalized = accuracy_score(y_test, y_pred, normalize=False)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    precision_avg = report['weighted avg']['precision']\n",
        "    recall_avg = report['weighted avg']['recall']\n",
        "    f1_score_avg = report['weighted avg']['f1-score']\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Calcular True Positive Rate(TPR) e True Negative Rate(TNR)\n",
        "\n",
        "    # TN -> True Negatives -> Número de verdadeiros negativos (Casos negativos\n",
        "    # identificados como negativos)\n",
        "\n",
        "    # TP -> True Positives -> Número de verdadeiros positivos (Casos positivos\n",
        "    # identificados como positivos)\n",
        "\n",
        "    # FP -> False Positives -> Número de falsos positivos (Casos negativos\n",
        "    # classificados como positivos)\n",
        "\n",
        "    # FN -> False Negatives -> Número de falsos negativos (Casos positivos\n",
        "    # classificados como negativos)\n",
        "    TN, FP, FN, TP = conf_matrix.ravel()\n",
        "    TPR = TP / (TP + FN)\n",
        "    TNR = TN / (TN + FP)\n",
        "\n",
        "    # Formatar a matriz de confusão\n",
        "    conf_matrix_df = pd.DataFrame(conf_matrix, index=['Real: Não-Spam', 'Real: Spam'], columns=['Previsto: Não-Spam', 'Previsto: Spam'])\n",
        "\n",
        "    # Calcular a pontuação composta\n",
        "    composed_score = (accuracy + precision_avg * 100 + recall_avg * 100 + f1_score_avg * 100 + TPR * 100 + TNR * 100) / 6\n",
        "\n",
        "    # Armazenar os resultados\n",
        "    results.append({\n",
        "        \"modelo\": model_name,\n",
        "        \"acuracia\": accuracy,\n",
        "        \"qt_acuracia\": accuracy_non_normalized,\n",
        "        # \"precision_avg\": precision_avg,\n",
        "        # \"recall_avg\": recall_avg,\n",
        "        # \"f1_score_avg\": f1_score_avg,\n",
        "        # \"TPR\": TPR,\n",
        "        # \"TNR\": TNR,\n",
        "        \"composed_score\": composed_score,\n",
        "        \"report\": report,\n",
        "        \"matrix\": conf_matrix_df,\n",
        "        \"y_test_shape\": y_test.shape[0],\n",
        "    })"
      ],
      "metadata": {
        "id": "B_gZv9pQjFmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordenar os resultados por maior pontuação composta\n",
        "results = sorted(results, key=lambda x: x[\"composed_score\"], reverse=True)\n",
        "\n",
        "# Exibir os resultados ordenados\n",
        "for result in results:\n",
        "    print(f\"Modelo: {result['modelo']}\")\n",
        "    print(f\"Acurácia: {result['acuracia']:.2f}%\")\n",
        "    print(f\"Acurácia (quantidade amostras): {result['qt_acuracia']} / {result['y_test_shape']}\")\n",
        "    # print(f\"Precisão Média: {result['precision_avg']:.2f}\")\n",
        "    # print(f\"Recall Médio: {result['recall_avg']:.2f}\")\n",
        "    # print(f\"F1-Score Médio: {result['f1_score_avg']:.2f}\")\n",
        "    # print(f\"Taxa de Verdadeiro Positivo (TPR): {result['TPR']:.2f}\")\n",
        "    # print(f\"Taxa de Verdadeiro Negativo (TNR): {result['TNR']:.2f}\")\n",
        "    print(f\"Pontuação Composta: {result['composed_score']:.2f}\")\n",
        "    print(\"Relatório de Classificação:\")\n",
        "    print(pd.DataFrame(result[\"report\"]).transpose())\n",
        "    print(\"Matriz de Confusão:\")\n",
        "    print(result[\"matrix\"])\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNebbtrjlqYQ",
        "outputId": "4b3ccfec-47cb-4a6b-d83f-2760a0ff6085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo: Random Forest\n",
            "Acurácia: 95.66%\n",
            "Acurácia (quantidade amostras): 881 / 921\n",
            "Pontuação Composta: 95.52\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.945554  0.981168  0.963031  531.000000\n",
            "1              0.972973  0.923077  0.947368  390.000000\n",
            "accuracy       0.956569  0.956569  0.956569    0.956569\n",
            "macro avg      0.959263  0.952122  0.955200  921.000000\n",
            "weighted avg   0.957164  0.956569  0.956399  921.000000\n",
            "Matriz de Confusão:\n",
            "                Previsto: Não-Spam  Previsto: Spam\n",
            "Real: Não-Spam                 521              10\n",
            "Real: Spam                      30             360\n",
            "\n",
            "============================================================\n",
            "\n",
            "Modelo: Neural Network\n",
            "Acurácia: 93.70%\n",
            "Acurácia (quantidade amostras): 863 / 921\n",
            "Pontuação Composta: 93.53\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.927667  0.966102  0.946494  531.000000\n",
            "1              0.951087  0.897436  0.923483  390.000000\n",
            "accuracy       0.937025  0.937025  0.937025    0.937025\n",
            "macro avg      0.939377  0.931769  0.934989  921.000000\n",
            "weighted avg   0.937584  0.937025  0.936750  921.000000\n",
            "Matriz de Confusão:\n",
            "                Previsto: Não-Spam  Previsto: Spam\n",
            "Real: Não-Spam                 513              18\n",
            "Real: Spam                      40             350\n",
            "\n",
            "============================================================\n",
            "\n",
            "Modelo: Support Vector Machine\n",
            "Acurácia: 93.49%\n",
            "Acurácia (quantidade amostras): 861 / 921\n",
            "Pontuação Composta: 93.30\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.924324  0.966102  0.944751  531.000000\n",
            "1              0.950820  0.892308  0.920635  390.000000\n",
            "accuracy       0.934853  0.934853  0.934853    0.934853\n",
            "macro avg      0.937572  0.929205  0.932693  921.000000\n",
            "weighted avg   0.935544  0.934853  0.934539  921.000000\n",
            "Matriz de Confusão:\n",
            "                Previsto: Não-Spam  Previsto: Spam\n",
            "Real: Não-Spam                 513              18\n",
            "Real: Spam                      42             348\n",
            "\n",
            "============================================================\n",
            "\n",
            "Modelo: Logistic Regression\n",
            "Acurácia: 91.97%\n",
            "Acurácia (quantidade amostras): 847 / 921\n",
            "Pontuação Composta: 91.77\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.911712  0.952919  0.931860  531.000000\n",
            "1              0.931694  0.874359  0.902116  390.000000\n",
            "accuracy       0.919653  0.919653  0.919653    0.919653\n",
            "macro avg      0.921703  0.913639  0.916988  921.000000\n",
            "weighted avg   0.920173  0.919653  0.919265  921.000000\n",
            "Matriz de Confusão:\n",
            "                Previsto: Não-Spam  Previsto: Spam\n",
            "Real: Não-Spam                 506              25\n",
            "Real: Spam                      49             341\n",
            "\n",
            "============================================================\n",
            "\n",
            "Modelo: Decision Tree\n",
            "Acurácia: 91.64%\n",
            "Acurácia (quantidade amostras): 844 / 921\n",
            "Pontuação Composta: 91.46\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.912727  0.945386  0.928770  531.000000\n",
            "1              0.921833  0.876923  0.898817  390.000000\n",
            "accuracy       0.916395  0.916395  0.916395    0.916395\n",
            "macro avg      0.917280  0.911155  0.913794  921.000000\n",
            "weighted avg   0.916583  0.916395  0.916086  921.000000\n",
            "Matriz de Confusão:\n",
            "                Previsto: Não-Spam  Previsto: Spam\n",
            "Real: Não-Spam                 502              29\n",
            "Real: Spam                      48             342\n",
            "\n",
            "============================================================\n",
            "\n",
            "Modelo: K-Nearest Neighbors\n",
            "Acurácia: 89.36%\n",
            "Acurácia (quantidade amostras): 823 / 921\n",
            "Pontuação Composta: 89.13\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.890090  0.930320  0.909761  531.000000\n",
            "1              0.898907  0.843590  0.870370  390.000000\n",
            "accuracy       0.893594  0.893594  0.893594    0.893594\n",
            "macro avg      0.894499  0.886955  0.890065  921.000000\n",
            "weighted avg   0.893824  0.893594  0.893081  921.000000\n",
            "Matriz de Confusão:\n",
            "                Previsto: Não-Spam  Previsto: Spam\n",
            "Real: Não-Spam                 494              37\n",
            "Real: Spam                      61             329\n",
            "\n",
            "============================================================\n",
            "\n",
            "Modelo: Naive Bayes\n",
            "Acurácia: 82.19%\n",
            "Acurácia (quantidade amostras): 757 / 921\n",
            "Pontuação Composta: 83.18\n",
            "Relatório de Classificação:\n",
            "              precision    recall  f1-score     support\n",
            "0              0.942169  0.736347  0.826638  531.000000\n",
            "1              0.723320  0.938462  0.816964  390.000000\n",
            "accuracy       0.821933  0.821933  0.821933    0.821933\n",
            "macro avg      0.832744  0.837404  0.821801  921.000000\n",
            "weighted avg   0.849497  0.821933  0.822542  921.000000\n",
            "Matriz de Confusão:\n",
            "                Previsto: Não-Spam  Previsto: Spam\n",
            "Real: Não-Spam                 391             140\n",
            "Real: Spam                      24             366\n",
            "\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}